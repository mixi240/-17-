# final_analysis.py - æœ€ç»ˆåˆ†æ
import json
import csv
from collections import Counter, defaultdict

print("å“ªå’çŸ¥è¯†å›¾è°± - æœ€ç»ˆåˆ†ææŠ¥å‘Š")
print("=" * 70)

# 1. åŸºæœ¬ä¿¡æ¯
print("\nğŸ“Š ä¸€ã€åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡")
print("-" * 40)

with open('output/statistics.json', 'r', encoding='utf-8') as f:
    stats = json.load(f)

print(f"â€¢ æ–‡æœ¬å¤„ç†é•¿åº¦: {stats.get('text_length', 'N/A'):,} å­—ç¬¦")
print(f"â€¢ è¯†åˆ«å®ä½“æ€»æ•°: {stats['total_entities']}")
print(f"â€¢ æŠ½å–å…³ç³»æ€»æ•°: {stats['total_relations']}")
print(f"â€¢ æ„å»ºä¸‰å…ƒç»„æ€»æ•°: {stats['total_triples']}")
print(f"â€¢ å“ªå’ç›¸å…³ä¸‰å…ƒç»„: {stats['nezha_triples']}")

# 2. å®ä½“åˆ†æ
print("\nğŸ‘¥ äºŒã€å®ä½“åˆ†æ")
print("-" * 40)

with open('output/entities.json', 'r', encoding='utf-8') as f:
    entities = json.load(f)

print(f"å…±è¯†åˆ« {len(entities)} ä¸ªå®ä½“")

# å®ä½“ç±»å‹åˆ†å¸ƒ
type_stats = defaultdict(int)
for entity in entities:
    type_stats[entity['type']] += 1

print("\nå®ä½“ç±»å‹åˆ†å¸ƒ:")
for etype, count in sorted(type_stats.items(), key=lambda x: x[1], reverse=True):
    print(f"  {etype:10} : {count:3d} ä¸ª")

# å‡ºç°é¢‘ç‡æœ€é«˜çš„å®ä½“
if 'count' in entities[0]:
    top_entities = sorted(entities, key=lambda x: x.get('count', 0), reverse=True)[:15]
    print("\nå‡ºç°é¢‘ç‡æœ€é«˜çš„å®ä½“:")
    for i, entity in enumerate(top_entities, 1):
        print(f"  {i:2d}. {entity['text']:15} : {entity.get('count', 0):4d} æ¬¡ ({entity['type']})")

# 3. å…³ç³»åˆ†æ
print("\nğŸ”— ä¸‰ã€å…³ç³»åˆ†æ")
print("-" * 40)

with open('output/relations.json', 'r', encoding='utf-8') as f:
    relations = json.load(f)

print(f"å…±æŠ½å– {len(relations)} ä¸ªå…³ç³»")

# å…³ç³»ç±»å‹åˆ†å¸ƒ
rel_stats = Counter([rel['predicate'] for rel in relations])
print("\nå…³ç³»ç±»å‹åˆ†å¸ƒ:")
for rel_type, count in rel_stats.most_common():
    print(f"  {rel_type:10} : {count:3d} ä¸ª")

# 4. å“ªå’æ ¸å¿ƒå…³ç³»ç½‘ç»œ
print("\nğŸŒŸ å››ã€å“ªå’æ ¸å¿ƒå…³ç³»ç½‘ç»œ")
print("-" * 40)

# è¯»å–å“ªå’ç›¸å…³ä¸‰å…ƒç»„
nezha_triples = []
try:
    with open('output/nezha_triples.csv', 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        nezha_triples = list(reader)
except:
    with open('output/nezha_triples.json', 'r', encoding='utf-8') as f:
        nezha_triples = json.load(f)

print(f"å“ªå’ç›¸å…³å…³ç³»: {len(nezha_triples)} ä¸ª")

# æŒ‰å…³ç³»ç±»å‹åˆ†ç»„
nezha_by_type = defaultdict(list)
for triple in nezha_triples:
    if isinstance(triple, dict):
        pred = triple.get('predicate') or triple.get('predicate', '')
        subject = triple.get('subject') or triple.get('subject', '')
        object_ = triple.get('object') or triple.get('object', '')
        nezha_by_type[pred].append((subject, object_))

print("\nå“ªå’å…³ç³»è¯¦ç»†åˆ†æ:")
for rel_type, pairs in sorted(nezha_by_type.items(), key=lambda x: len(x[1]), reverse=True):
    print(f"\nã€{rel_type.upper()}ã€‘å…³ç³» ({len(pairs)}ä¸ª):")
    for subject, object_ in pairs[:8]:  # æ¯ç§ç±»å‹æœ€å¤šæ˜¾ç¤º8ä¸ª
        print(f"  â€¢ {subject} â†’ {object_}")

# 5. çŸ¥è¯†å‘ç°æ€»ç»“
print("\nğŸ’¡ äº”ã€é‡è¦çŸ¥è¯†å‘ç°")
print("-" * 40)

# æ£€æŸ¥å…³é”®çŸ¥è¯†æ˜¯å¦è¢«æå–
key_knowledge = [
    ("å“ªå’", "çˆ¶äº²", "æé–", "å®¶åº­å…³ç³»"),
    ("å“ªå’", "å¸ˆçˆ¶", "å¤ªä¹™çœŸäºº", "å¸ˆå¾’å…³ç³»"),
    ("å“ªå’", "å‡ºç°äº", "ã€Šå°ç¥æ¼”ä¹‰ã€‹", "ä½œå“å½’å±"),
    ("å´æ‰¿æ©", "åˆ›ä½œ", "ã€Šè¥¿æ¸¸è®°ã€‹", "æ–‡å­¦åˆ›ä½œ"),
    ("è®¸ä»²ç³", "åˆ›ä½œ", "ã€Šå°ç¥æ¼”ä¹‰ã€‹", "æ–‡å­¦åˆ›ä½œ"),
    ("é¥ºå­", "å¯¼æ¼”", "ã€Šå“ªå’ä¹‹é­”ç«¥é™ä¸–ã€‹", "å½±è§†åˆ›ä½œ"),
    ("ã€Šå“ªå’ä¹‹é­”ç«¥é™ä¸–ã€‹", "æ”¹ç¼–è‡ª", "ã€Šå°ç¥æ¼”ä¹‰ã€‹", "ä½œå“æ”¹ç¼–"),
    ("å“ªå’", "æ•Œäºº", "æ•–ä¸™", "æ•Œå¯¹å…³ç³»"),
    ("å“ªå’", "æœ‹å‹", "æ•–ä¸™", "æœ‹å‹å…³ç³»"),
]

found_count = 0
print("å…³é”®çŸ¥è¯†æ£€æŸ¥:")
for subject, predicate, object_, desc in key_knowledge:
    found = False
    for triple in nezha_triples:
        if isinstance(triple, dict):
            t_subj = triple.get('subject') or triple.get('subject', '')
            t_pred = triple.get('predicate') or triple.get('predicate', '')
            t_obj = triple.get('object') or triple.get('object', '')
        else:
            t_subj, t_pred, t_obj = triple[0], triple[1], triple[2]
        
        if t_subj == subject and t_pred == predicate and t_obj == object_:
            found = True
            break
    
    if found:
        print(f"  âœ“ {desc}: {subject} --[{predicate}]--> {object_}")
        found_count += 1
    else:
        print(f"  â—‹ {desc}: {subject} --[{predicate}]--> {object_}")

print(f"\nçŸ¥è¯†å‘ç°ç‡: {found_count}/{len(key_knowledge)} ({found_count/len(key_knowledge)*100:.1f}%)")

# 6. å¯¼å‡ºå»ºè®®
print("\nğŸ“ å…­ã€ç»“æœæ–‡ä»¶è¯´æ˜")
print("-" * 40)

files_info = [
    ("triples.csv", "æ‰€æœ‰ä¸‰å…ƒç»„", "Excelå¯æ‰“å¼€ï¼Œå®Œæ•´çŸ¥è¯†å›¾è°±"),
    ("nezha_triples.csv", "å“ªå’ç›¸å…³ä¸‰å…ƒç»„", "æ ¸å¿ƒåˆ†æå¯¹è±¡"),
    ("entities.json", "æ‰€æœ‰å®ä½“", "JSONæ ¼å¼ï¼Œå®Œæ•´å®ä½“åˆ—è¡¨"),
    ("relations.json", "æ‰€æœ‰å…³ç³»", "JSONæ ¼å¼ï¼Œå®Œæ•´å…³ç³»åˆ—è¡¨"),
    ("statistics.json", "ç»Ÿè®¡ä¿¡æ¯", "JSONæ ¼å¼ï¼Œå„ç±»ç»Ÿè®¡"),
    ("report.txt", "æ–‡æœ¬æŠ¥å‘Š", "ç®€è¦åˆ†ææŠ¥å‘Š"),
]

print("ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨:")
for filename, name, desc in files_info:
    print(f"  â€¢ {filename:20} - {name:15} : {desc}")

# 7. åç»­ç ”ç©¶å»ºè®®
print("\nğŸ”¬ ä¸ƒã€åç»­ç ”ç©¶æ–¹å‘å»ºè®®")
print("-" * 40)

suggestions = [
    "1. æ‰©å±•å®ä½“ç±»å‹ï¼šæ·»åŠ æ›´å¤šç¥è¯äººç‰©ã€åœ°ç‚¹ã€æ¦‚å¿µ",
    "2. æ·±åŒ–å…³ç³»æŠ½å–ï¼šå¢åŠ æ—¶é—´å…³ç³»ã€å½±å“å…³ç³»ã€å¯¹æ¯”å…³ç³»",
    "3. æ„å»ºæ—¶é—´çº¿ï¼šåˆ†æå“ªå’å½¢è±¡çš„å†å²æ¼”å˜",
    "4. è·¨ä½œå“åˆ†æï¼šæ¯”è¾ƒä¸åŒä½œå“ä¸­çš„å“ªå’å½¢è±¡å·®å¼‚",
    "5. å­¦è€…ç½‘ç»œï¼šåˆ†æç ”ç©¶å“ªå’çš„å­¦è€…åŠå…¶è§‚ç‚¹",
    "6. å¯è§†åŒ–å±•ç¤ºï¼šä½¿ç”¨Neo4jæˆ–Gephiå¯è§†åŒ–çŸ¥è¯†å›¾è°±",
]

for suggestion in suggestions:
    print(suggestion)

print("\n" + "=" * 70)
print("åˆ†æå®Œæˆï¼å“ªå’çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸï¼ğŸ‰")
print("=" * 70)

# ä¿å­˜è¯¦ç»†æŠ¥å‘Š
report_data = {
    "summary": {
        "total_entities": stats['total_entities'],
        "total_relations": stats['total_relations'],
        "total_triples": stats['total_triples'],
        "nezha_relations": len(nezha_triples)
    },
    "entity_types": dict(type_stats),
    "relation_types": dict(rel_stats),
    "nezha_network": {
        rel_type: pairs for rel_type, pairs in nezha_by_type.items()
    },
    "key_findings": {
        "found": found_count,
        "total": len(key_knowledge),
        "rate": f"{found_count/len(key_knowledge)*100:.1f}%"
    }
}

with open('output/detailed_report.json', 'w', encoding='utf-8') as f:
    json.dump(report_data, f, ensure_ascii=False, indent=2)

print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: output/detailed_report.json")