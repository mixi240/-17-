# analyze_results.py - åˆ†æç»“æœ
import json
import csv
from collections import Counter, defaultdict

print("å“ªå’çŸ¥è¯†å›¾è°±åˆ†ææŠ¥å‘Š")
print("=" * 60)

# 1. åŠ è½½ç»Ÿè®¡ä¿¡æ¯
with open('output/statistics.json', 'r', encoding='utf-8') as f:
    stats = json.load(f)

print(f"ğŸ“Š æ€»å®ä½“æ•°: {stats['total_entities']}")
print(f"ğŸ”— æ€»å…³ç³»æ•°: {stats['total_relations']}")
print(f"ğŸ“‹ æ€»ä¸‰å…ƒç»„: {stats['total_triples']}")
print(f"ğŸŒŸ å“ªå’ç›¸å…³: {stats['nezha_triples']}")

# 2. åˆ†æå“ªå’ç›¸å…³å…³ç³»
print("\n" + "=" * 60)
print("å“ªå’çš„æ ¸å¿ƒå…³ç³»ç½‘ç»œ")
print("=" * 60)

nezha_relations = []
with open('output/nezha_triples.csv', 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    for row in reader:
        nezha_relations.append(row)

# æŒ‰å…³ç³»ç±»å‹åˆ†ç»„
relation_groups = defaultdict(list)
for rel in nezha_relations:
    relation_groups[rel['predicate']].append(rel)

print(f"å‘ç° {len(nezha_relations)} ä¸ªå“ªå’ç›¸å…³å…³ç³»ï¼Œæ¶‰åŠ {len(relation_groups)} ç§å…³ç³»ç±»å‹")

for rel_type, rels in sorted(relation_groups.items(), key=lambda x: len(x[1]), reverse=True):
    print(f"\nã€{rel_type.upper()}ã€‘å…³ç³» ({len(rels)}ä¸ª):")
    for rel in rels[:5]:  # æ¯ç§ç±»å‹æ˜¾ç¤ºå‰5ä¸ª
        print(f"  â€¢ {rel['subject']} â†’ {rel['object']}")

# 3. åˆ†æå®ä½“é‡è¦æ€§
print("\n" + "=" * 60)
print("å…³é”®å®ä½“åˆ†æ")
print("=" * 60)

# åŠ è½½å®ä½“
with open('output/entities.json', 'r', encoding='utf-8') as f:
    entities = json.load(f)

# æŒ‰å‡ºç°é¢‘ç‡æ’åº
sorted_entities = sorted(entities, key=lambda x: x.get('count', 0), reverse=True)

print("å‡ºç°é¢‘ç‡æœ€é«˜çš„å®ä½“:")
for i, entity in enumerate(sorted_entities[:15], 1):
    print(f"  {i:2d}. {entity['text']:15} ({entity['type']}): {entity.get('count', 'N/A')} æ¬¡")

# 4. å‘ç°çš„é‡è¦çŸ¥è¯†
print("\n" + "=" * 60)
print("é‡è¦çŸ¥è¯†å‘ç°")
print("=" * 60)

important_findings = []

# æ£€æŸ¥å“ªäº›å…³é”®å…³ç³»è¢«å‘ç°äº†
key_relations_to_check = [
    ("å“ªå’", "çˆ¶äº²", "æé–"),
    ("å“ªå’", "å¸ˆçˆ¶", "å¤ªä¹™çœŸäºº"),
    ("å“ªå’", "æœ‹å‹", "æ•–ä¸™"),
    ("å“ªå’", "æ•Œäºº", "æ•–ä¸™"),
    ("å´æ‰¿æ©", "åˆ›ä½œ", "ã€Šè¥¿æ¸¸è®°ã€‹"),
    ("è®¸ä»²ç³", "åˆ›ä½œ", "ã€Šå°ç¥æ¼”ä¹‰ã€‹"),
    ("é¥ºå­", "å¯¼æ¼”", "ã€Šå“ªå’ä¹‹é­”ç«¥é™ä¸–ã€‹"),
    ("ã€Šå“ªå’ä¹‹é­”ç«¥é™ä¸–ã€‹", "æ”¹ç¼–è‡ª", "ã€Šå°ç¥æ¼”ä¹‰ã€‹"),
]

found_count = 0
for subj, pred, obj in key_relations_to_check:
    found = False
    for rel in nezha_relations:
        if rel['subject'] == subj and rel['predicate'] == pred and rel['object'] == obj:
            found = True
            break
    
    if found:
        print(f"âœ“ å‘ç°: {subj} --[{pred}]--> {obj}")
        found_count += 1
    else:
        print(f"â—‹ æœªå‘ç°: {subj} --[{pred}]--> {obj}")

print(f"\nå…³é”®å…³ç³»å‘ç°ç‡: {found_count}/{len(key_relations_to_check)}")

# 5. ç”ŸæˆçŸ¥è¯†ç½‘ç»œæ‘˜è¦
print("\n" + "=" * 60)
print("çŸ¥è¯†ç½‘ç»œæ‘˜è¦")
print("=" * 60)

# ç»Ÿè®¡ä¸å“ªå’ç›´æ¥ç›¸å…³çš„å®ä½“
connected_to_nezha = set()
for rel in nezha_relations:
    if rel['subject'] == 'å“ªå’':
        connected_to_nezha.add(rel['object'])
    elif rel['object'] == 'å“ªå’':
        connected_to_nezha.add(rel['subject'])

print(f"ä¸å“ªå’ç›´æ¥ç›¸å…³çš„å®ä½“æœ‰ {len(connected_to_nezha)} ä¸ª:")
for i, entity in enumerate(sorted(connected_to_nezha)[:20], 1):
    print(f"  {i:2d}. {entity}")

# 6. ä¿å­˜è¯¦ç»†åˆ†æ
analysis_results = {
    "summary": {
        "total_entities": stats['total_entities'],
        "total_relations": stats['total_relations'],
        "total_triples": stats['total_triples'],
        "nezha_relations": len(nezha_relations)
    },
    "key_findings": [
        f"å‘ç° {len(nezha_relations)} ä¸ªå“ªå’ç›¸å…³å…³ç³»",
        f"æ¶‰åŠ {len(connected_to_nezha)} ä¸ªä¸å“ªå’ç›´æ¥ç›¸å…³çš„å®ä½“",
        f"å…³é”®å…³ç³»å‘ç°ç‡: {found_count}/{len(key_relations_to_check)}"
    ],
    "top_entities": [e['text'] for e in sorted_entities[:10]],
    "nezha_network": list(connected_to_nezha)
}

with open('output/detailed_analysis.json', 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, ensure_ascii=False, indent=2)

print(f"\nâœ“ è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ°: output/detailed_analysis.json")
print("\n" + "=" * 60)
print("åˆ†æå®Œæˆï¼")
print("=" * 60)